<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SemGes: Semantic-Aware Gesture Generation</title>

    <style>
        @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&family=Georgia&display=swap');

        body {
            font-family: 'Playfair Display', Georgia, serif;
            font-weight: 400;
            font-size: 18px;
            margin-left: auto;
            margin-right: auto;
            width: 1100px;
            background-color: #f9f9f9;
        }

        h1 {
            font-size: 36px;
            font-weight: 700;
            text-align: center;
            color: #333;
        }

        .video-container {
            text-align: center;
            margin: 20px 0;
        }

        video {
            width: 80%;
            max-width: 800px;
            border-radius: 12px;
            border: 2px solid #ccc;
            box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.2);
        }

        .video-caption {
            font-size: 20px;
            color: #555;
            font-style: italic;
            margin-top: 10px;
        }

        .content {
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
        }
    </style>
</head>
<body>

    <br>
    <center>
        <h1>SemGes: Semantic-Aware Gesture Generation through Semantic Coherence and Relevance Learning</h1>
    </center>

    <div class="video-container">
        <video controls>
            <source src="your-video-file.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <div class="video-caption">
            Figure 1: Example of generated gestures from our model.
        </div>
    </div>

    <div class="content">
        <h1>Abstract</h1>
        <p>
            Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task.
            Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures.
            In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels.
            Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics, and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules.
            Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures.
            Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics.
        </p>
    </div>

</body>
</html>
