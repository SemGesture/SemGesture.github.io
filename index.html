<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SemGes: Semantic-Aware Gesture Generation</title>

    <!-- Swiper.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@9/swiper-bundle.min.css">

    <style>
        @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&display=swap');

        body {
            font-family: 'Playfair Display', serif;
            font-size: 18px;
            margin-left: auto;
            margin-right: auto;
            width: 1100px;
            background-color: #f9f9f9;
        }

        h1 {
            font-size: 36px;
            font-weight: 700;
            text-align: center;
            color: #333;
        }

        /* Swiper styles */
        .swiper-container {
            width: 100%;
            max-width: 900px;
            margin: auto;
            padding-top: 20px;
            padding-bottom: 40px;
        }

        .swiper-slide {
            display: flex;
            justify-content: center;
            align-items: center;
            background: #fff;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.2);
        }

        video {
            width: 100%;
            max-width: 600px;
            border-radius: 12px;
            border: 2px solid #ccc;
        }

        .video-caption {
            text-align: center;
            font-size: 16px;
            color: #555;
            font-style: italic;
            margin-top: 5px;
        }

        .content {
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
            margin-top: 20px;
        }

        /* Highlights section */
        .highlights {
            text-align: center;
            background-color: #fff;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
            margin: 20px 0;
        }

        .highlights h2 {
            font-size: 28px;
            font-weight: bold;
            color: #444;
        }

        .highlights p {
            font-size: 18px;
            color: #333;
            margin-bottom: 10px;
        }

        .highlights strong {
            color: #b22222;
        }

        .highlights .green {
            color: #228B22;
        }

        .highlights .golden {
            color: #B8860B;
        }

        .highlights .blue {
            color: #1E3A8A;
        }

        .highlights img {
            width: 80%;
            max-width: 800px;
            border-radius: 10px;
            margin-top: 15px;
        }
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">

</head>
<body>

    <br>
    <center>
        <!-- Title -->
        <h1 style="font-size: 32px; font-weight: 700; font-family: 'Helvetica', 'Arial', sans-serif; color: #222;">
          SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning
        </h1>
      
        <!-- Authors with links -->
        <p style="font-size: 20px; font-weight: 500; font-family: 'Helvetica', 'Arial', sans-serif; line-height: 1.6; margin-top: 10px;">
          <a href="https://www.mpi.nl/people/liu-lanmiao" target="_blank" style="color: #1D9BF0; text-decoration: none;">Lanmiao Liu</a><sup>1,2,3</sup>, 
          <a href="https://esamghaleb.github.io/" target="_blank" style="color: #1D9BF0; text-decoration: none;">Esam Ghaleb</a><sup>1,2</sup>, 
          <a href="https://www.mpi.nl/people/ozyurek-asli" target="_blank" style="color: #1D9BF0; text-decoration: none;">Aslƒ± √ñzy√ºrek</a><sup>1,2</sup>, 
          <a href="https://www.uu.nl/staff/ZYumak" target="_blank" style="color: #1D9BF0; text-decoration: none;">Zerrin Yumak</a><sup>3</sup>
        </p>
      
        <!-- Affiliations -->
        <p style="font-size: 16px; color: #444; font-family: 'Helvetica', 'Arial', sans-serif; margin: 5px 0;">
          <sup>1</sup>Max Planck Institute for Psycholinguistics, 
          <sup>2</sup>Donders Institute for Brain, Cognition and Behaviour, 
          <sup>3</sup>Utrecht University
        </p>
      
        <!-- Emails -->
        <p style="font-size: 15px; color: #555; font-family: 'Courier New', Courier, monospace;">
          {lanmiao.liu, esam.ghaleb, asli.ozyurek}@mpi.nl &nbsp;&nbsp;&nbsp; z.yumak@uu.nl
        </p>

        <!-- Conference Tag -->
        <p style="font-size: 16px; color: #B22222; font-weight: bold; font-family: 'Helvetica', sans-serif; margin-top: 5px;">
            üèÜ Accepted at <span style="color: #1D9BF0;">ICCV 2025</span>
        </p>

      </center>
      
    <div style="display: flex; justify-content: center; gap: 12px; margin-top: 30px; flex-wrap: wrap;">

    <!-- arXiv Button -->
    <a href="https://www.arxiv.org/abs/2507.19359" target="_blank" style="
        background-color: #2e2e2e;
        color: white;
        text-decoration: none;
        padding: 10px 18px;
        border-radius: 24px;
        font-family: 'Helvetica', sans-serif;
        font-size: 16px;
        display: flex;
        align-items: center;
        gap: 8px;
        transition: background 0.2s ease;
    " onmouseover="this.style.backgroundColor='#444'" onmouseout="this.style.backgroundColor='#2e2e2e'">
        <i class="fas fa-code-branch"></i> arXiv
    </a>
    
    <!-- GitHub Button -->
    <a href="https://github.com/marcos452/SemGes" target="_blank" style="
        background-color: #2e2e2e;
        color: white;
        text-decoration: none;
        padding: 10px 18px;
        border-radius: 24px;
        font-family: 'Helvetica', sans-serif;
        font-size: 16px;
        display: flex;
        align-items: center;
        gap: 8px;
        transition: background 0.2s ease;
    " onmouseover="this.style.backgroundColor='#444'" onmouseout="this.style.backgroundColor='#2e2e2e'">
        <i class="fab fa-github"></i> GitHub
    </a>
    
    </div>
      
    

    <!-- Swiper for videos (above abstract) -->
    <div class="swiper-container swiper">
        <div class="swiper-wrapper">
            <div class="swiper-slide">
                <video controls>
                    <source src="video/SemGes.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        <!-- Pagination & navigation -->
        <div class="swiper-pagination"></div>
        <div class="swiper-button-prev"></div>
        <div class="swiper-button-next"></div>
    </div>

    <div class="video-caption">Method Overview</div>
    <div class="video-caption">(Hint: The video plays smoothly in Opera and Chrome browsers.)</div>

    <!-- Highlights Section -->
    <div class="highlights">
        <h2>üî• Highlights</h2>
        <p>We introduce a novel framework, <strong>SemGes</strong>, that first learns a robust VQ-VAE motion prior for body and hand gestures, and then generates gestures driven by fused speech audio, text-based semantics, and speaker identity in a cross-modal transformer.</p>
        <p>Our method jointly captures <strong class="golden">discourse-level context</strong> via a semantic coherence loss and <span class="green">fine-grained representational gestures</span> (e.g., iconic, metaphoric) via a semantic relevance loss.</p>
        <p>We propose an <strong class="blue">overlap-and-combine inference algorithm</strong> that maintains smooth continuity over extended durations.</p>
        <p>Extensive experiments on two benchmarks, namely, the <span class="golden">BEAT</span> and <span class="golden">TED Expressive</span> datasets show that our method outperforms recent baselines in both objective metrics (e.g., Fr√©chet Gesture Distance (FGD), diversity, semantic alignment) and user judgment of generated gestures.</p>
    </div>

    <div class="content">
        <h1>üìÑ Abstract</h1>
        <p>Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics.</p>
    </div>
	<!DOCTYPE html>
	<html lang="en">
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>SemGes: Semantic-Aware Gesture Generation</title>
	
		<!-- Swiper.js CSS -->
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@9/swiper-bundle.min.css">
	
		<style>
			@import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&display=swap');
	
			body {
				font-family: 'Playfair Display', serif;
				font-size: 18px;
				margin-left: auto;
				margin-right: auto;
				width: 1100px;
				background-color: #f9f9f9;
			}
	
			h1 {
				font-size: 36px;
				font-weight: 700;
				text-align: center;
				color: #333;
			}
	
			/* Swiper styles */
			.swiper-container {
				width: 100%;
				max-width: 900px;
				margin: auto;
				padding-top: 20px;
				padding-bottom: 40px;
			}
	
			.swiper-slide {
				display: flex;
				justify-content: center;
				align-items: center;
				background: #fff;
				border-radius: 10px;
				overflow: hidden;
				box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.2);
			}
	
			video {
				width: 100%;
				max-width: 600px;
				border-radius: 12px;
				border: 2px solid #ccc;
			}
	
			.video-caption {
				text-align: center;
				font-size: 16px;
				color: #555;
				font-style: italic;
				margin-top: 5px;
			}
	
			.content {
				background: white;
				padding: 20px;
				border-radius: 10px;
				box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
				margin-top: 20px;
			}
	
			/* Highlights section */
			.highlights {
				text-align: center;
				background-color: #fff;
				padding: 20px;
				border-radius: 10px;
				box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
				margin: 20px 0;
			}
	
			.highlights h2 {
				font-size: 28px;
				font-weight: bold;
				color: #444;
			}
	
			.highlights p {
				font-size: 18px;
				color: #333;
				margin-bottom: 10px;
			}
	
			.framework {
            text-align: center;
            background-color: #fff;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
            margin-top: 20px;
			}

			.framework h2 {
				font-size: 28px;
				font-weight: bold;
				color: #444;
			}

			.framework p {
				font-size: 18px;
				color: #333;
				margin-bottom: 10px;
			}

			.framework img {
				width: 100%;
				max-width: 100%;
				height: auto;
				border-radius: 10px;
				margin-top: 15px;
			}
		</style>
	</head>
	<body>

	
		<!-- Framework Section -->
		<div class="framework">
			<h2>üõ†Ô∏è SemGes Framework Overview</h2>
			<img src="resources/pipeline.png" alt="SemGes Framework">
			<p>SemGes employs three training pathways:</p>
			<p><strong>(1) Global semantic coherence</strong>, which minimizes latent disparities between gesture and text encoders.</p>
			<p><strong>(2) Multimodal Quantization learning</strong>, where integrated multimodal representation codes are aligned with quantized motion to decode them into hand and body movements.</p>
			<p><strong>(3) Semantic relevance learning</strong>, which emphasizes semantic gestures.</p>

		</div>
	</body>
	</html>

		<!-- Main Results Section -->
		<div class="framework">
			<h2>üìä Main Results</h2>
			<p>Our model achieves state-of-the-art performance across multiple benchmarks with significant improvements in 
				BEAT and TED Expressive datasets .</p>
			<img src="resources/res.png" alt="SemGes Framework">

		</div>
	</body>
	</html>


		<!-- User Study Section -->
		<div class="framework">
			<h2>üë• User Study</h2>
        <p>We conducted extensive user studies to evaluate the naturalness, diversity, and alignment with speech content and timing of our generated gestures. The results confirm that our model outperforms existing approaches.</p>
			<img src="resources/user.png" alt="SemGes Framework">
		</div>
	</body>
	</html>
    

    <!-- Swiper for videos (above abstract) -->
            <div class="swiper-container swiper">
                <div class="swiper-wrapper">
                    <div class="swiper-slide">
                        <video controls>
                            <source src="video/4.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="swiper-slide">
                        <video controls>
                            <source src="video/5.mp4" type="video/mp4">
                        </video>
                    </div>
                    <!-- <div class="swiper-slide">
                        <video controls>
                            <source src="video/6.mp4" type="video/mp4">
                        </video>
                    </div> -->
                </div>
                <!-- Pagination & navigation -->
                <div class="swiper-pagination"></div>
                <div class="swiper-button-prev"></div>
                <div class="swiper-button-next"></div>
            </div>

            <div class="video-caption">SemGes: semantically generated gestures from our model.</div>
	</body>
	</html>

    <!-- Acknowledgement Section -->
    <div class="framework">
        <h2>üôè Acknowledgements</h2>
        <p>We would like to sincerely thank <strong>Sachit Mirsha</strong> for his generous support in rendering the avatar animations used in this project.</p>
    </div>

    <!-- Citation Section -->
    <div class="framework">
        <h2>üìö Citation</h2>
        <p>If you find our work useful for your research, please consider citing:</p>
        
        <div style="background-color: #f4f4f4; padding: 15px 20px; border-radius: 10px; overflow-x: auto; text-align: left;">
            <pre style="margin: 0; font-size: 14px;">
                @inproceedings{liu2025semges,
                    title     = {SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning},
                    author    = {Lanmiao Liu and Esam Ghaleb and Aslƒ± √ñzy√ºrek and Zerrin Yumak},
                    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
                    year      = {2025},
                    month     = oct,
                    address   = {Honolulu, Hawai‚Äòi, USA},
                    publisher = {IEEE},
                    url       = {https://semgesture.github.io/},
                    note      = {To appear},
                    series    = {ICCV '25'}
                  }
            </pre>
        </div>
    </div>

            


    
    
	
    <!-- Swiper.js Script -->
    <script src="https://cdn.jsdelivr.net/npm/swiper@9/swiper-bundle.min.js"></script>
    <script>
        var swiper = new Swiper(".swiper", {
            slidesPerView: 1,
            spaceBetween: 10,
            loop: true,
            navigation: {
                nextEl: ".swiper-button-next",
                prevEl: ".swiper-button-prev",
            },
            pagination: {
                el: ".swiper-pagination",
                clickable: true,
            },
        });
    </script>

</body>
</html>
